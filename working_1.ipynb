{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"full_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entity_column(col):\n",
    "    # Remove digits after commas (e.g., \"John Doe,123\" → \"John Doe\")\n",
    "    return col.apply(lambda x: [re.sub(r',\\d+', '', ent) for ent in str(x).split(';') if ent.strip()])\n",
    "\n",
    "# Clean each entity column\n",
    "df['persons'] = clean_entity_column(df['persons'])\n",
    "df['organizations'] = clean_entity_column(df['organizations'])\n",
    "df['locations'] = clean_entity_column(df['locations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>persons</th>\n",
       "      <th>organizations</th>\n",
       "      <th>locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"articleBody\":\"A federal judge has ruled again...</td>\n",
       "      <td>[Louie Gohmert, Timothy Kelly, Andrew Clyde, L...</td>\n",
       "      <td>[Dc District Court]</td>\n",
       "      <td>[Georgia,Pennsylvania,Texas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"articleBody\":\"More than a dozen major news or...</td>\n",
       "      <td>[Laura Lee Prather, Haynes Boone, Nicole Carroll]</td>\n",
       "      <td>[Texas Department Of Public Safety, Texas Depa...</td>\n",
       "      <td>[Robb Elementary School,Texas Department Of Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"articleBody\":\"Comedian Jon Stewart and vetera...</td>\n",
       "      <td>[Pat Toomey, Kate Bolduan, Matt Zeller, Jon St...</td>\n",
       "      <td>[Senate Majority Leader Chuck Schumer, While S...</td>\n",
       "      <td>[Iraq,America,Pennsylvania]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"articleBody\":\"A federal judge has ruled again...</td>\n",
       "      <td>[Louie Gohmert, Timothy Kelly, Andrew Clyde, L...</td>\n",
       "      <td>[Dc District Court]</td>\n",
       "      <td>[Georgia,Pennsylvania,Texas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"articleBody\":\"A version of this story appears...</td>\n",
       "      <td>[Pat Toomey, Joe Manchin, Paul Leblanc, Jake T...</td>\n",
       "      <td>[Union On, Senate Republicans, Veterans Affair...</td>\n",
       "      <td>[Pennsylvania,Capitol Hill,West Virginia,Ameri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  \"articleBody\":\"A federal judge has ruled again...   \n",
       "1  \"articleBody\":\"More than a dozen major news or...   \n",
       "2  \"articleBody\":\"Comedian Jon Stewart and vetera...   \n",
       "3  \"articleBody\":\"A federal judge has ruled again...   \n",
       "4  \"articleBody\":\"A version of this story appears...   \n",
       "\n",
       "                                             persons  \\\n",
       "0  [Louie Gohmert, Timothy Kelly, Andrew Clyde, L...   \n",
       "1  [Laura Lee Prather, Haynes Boone, Nicole Carroll]   \n",
       "2  [Pat Toomey, Kate Bolduan, Matt Zeller, Jon St...   \n",
       "3  [Louie Gohmert, Timothy Kelly, Andrew Clyde, L...   \n",
       "4  [Pat Toomey, Joe Manchin, Paul Leblanc, Jake T...   \n",
       "\n",
       "                                       organizations  \\\n",
       "0                                [Dc District Court]   \n",
       "1  [Texas Department Of Public Safety, Texas Depa...   \n",
       "2  [Senate Majority Leader Chuck Schumer, While S...   \n",
       "3                                [Dc District Court]   \n",
       "4  [Union On, Senate Republicans, Veterans Affair...   \n",
       "\n",
       "                                           locations  \n",
       "0                       [Georgia,Pennsylvania,Texas]  \n",
       "1  [Robb Elementary School,Texas Department Of Pu...  \n",
       "2                        [Iraq,America,Pennsylvania]  \n",
       "3                       [Georgia,Pennsylvania,Texas]  \n",
       "4  [Pennsylvania,Capitol Hill,West Virginia,Ameri...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"text\", \"persons\", \"organizations\",\"locations\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification)\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from seqeval.metrics import classification_report as seq_classification_report\n",
    "from seqeval.metrics import f1_score as seq_f1_score\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, tags, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Create tag to id mapping\n",
    "        unique_tags = set()\n",
    "        for tag_list in tags:\n",
    "            unique_tags.update(tag_list)\n",
    "        \n",
    "        self.tag2id = {tag: i for i, tag in enumerate(sorted(unique_tags))}\n",
    "        self.id2tag = {i: tag for tag, i in self.tag2id.items()}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tags = self.tags[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True,\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "        \n",
    "        # Align tags with tokens\n",
    "        labels = []\n",
    "        word_ids = encoding.word_ids()\n",
    "        previous_word_idx = None\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                if word_idx < len(tags):\n",
    "                    labels.append(self.tag2id[tags[word_idx]])\n",
    "                else:\n",
    "                    labels.append(self.tag2id['O'])\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        encoding['labels'] = labels\n",
    "        del encoding['offset_mapping']\n",
    "        \n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "\n",
    "class ExperimentTracker:\n",
    "    def __init__(self, experiment_dir='experiments'):\n",
    "        self.experiment_dir = experiment_dir\n",
    "        os.makedirs(experiment_dir, exist_ok=True)\n",
    "        self.experiments = []\n",
    "        \n",
    "    def log_experiment(self, experiment_name: str, model_name: str, \n",
    "                      hyperparameters: dict, metrics: dict, notes: str = \"\"):\n",
    "        experiment = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'experiment_name': experiment_name,\n",
    "            'model_name': model_name,\n",
    "            'hyperparameters': hyperparameters,\n",
    "            'metrics': metrics,\n",
    "            'notes': notes\n",
    "        }\n",
    "        \n",
    "        self.experiments.append(experiment)\n",
    "        \n",
    "        # Save to file\n",
    "        with open(os.path.join(self.experiment_dir, 'experiments.json'), 'w') as f:\n",
    "            json.dump(self.experiments, f, indent=2)\n",
    "            \n",
    "        print(f\"Experiment '{experiment_name}' logged successfully\")\n",
    "        \n",
    "    def get_best_experiment(self, metric='f1_score'):\n",
    "        if not self.experiments:\n",
    "            return None\n",
    "        return max(self.experiments, key=lambda x: x['metrics'].get(metric, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paicr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\paicr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def create_bio_format(df):\n",
    "    all_data = []\n",
    "    sentence_id = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        tokens = word_tokenize(text)\n",
    "        tags = ['O'] * len(tokens)\n",
    "\n",
    "        def tag_entities(entities, label_prefix):\n",
    "            for entity in entities:\n",
    "                entity_tokens = word_tokenize(entity)\n",
    "                for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "                    if tokens[i:i + len(entity_tokens)] == entity_tokens:\n",
    "                        tags[i] = f'B-{label_prefix}'\n",
    "                        for j in range(1, len(entity_tokens)):\n",
    "                            tags[i + j] = f'I-{label_prefix}'\n",
    "\n",
    "        tag_entities(row['persons'], 'PER')\n",
    "        tag_entities(row['organizations'], 'ORG')\n",
    "        tag_entities(row['locations'], 'LOC')\n",
    "\n",
    "        for token, tag in zip(tokens, tags):\n",
    "            all_data.append({\n",
    "                'sentence_id': sentence_id,\n",
    "                'word': token,\n",
    "                'tag': tag\n",
    "            })\n",
    "\n",
    "        sentence_id += 1\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "bio_df = create_bio_format(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel:\n",
    "    def __init__(self,experiment_name : str, model_name: str, experiment_tracker: ExperimentTracker):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.experiment_tracker = experiment_tracker\n",
    "        \n",
    "    def prepare_data(self, bio_df: pd.DataFrame):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Group by sentence_id\n",
    "        sentences = []\n",
    "        sentence_tags = []\n",
    "        \n",
    "        for sent_id in bio_df['sentence_id'].unique():\n",
    "            sent_data = bio_df[bio_df['sentence_id'] == sent_id]\n",
    "            sentences.append(sent_data['word'].tolist())\n",
    "            sentence_tags.append(sent_data['tag'].tolist())\n",
    "        \n",
    "        return sentences, sentence_tags\n",
    "    \n",
    "    def train_model(self, train_texts, train_tags, val_texts, val_tags):\n",
    "        \"\"\"Train baseline BERT model\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = NERDataset(train_texts, train_tags, self.tokenizer)\n",
    "        val_dataset = NERDataset(val_texts, val_tags, self.tokenizer)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=len(train_dataset.tag2id),\n",
    "            id2label=train_dataset.id2tag,\n",
    "            label2id=train_dataset.tag2id\n",
    "        )\n",
    "\n",
    "        def compute_metrics(p):\n",
    "            \"\"\"Computes F1, precision, and recall for seqeval.\"\"\"\n",
    "            predictions, labels = p\n",
    "            predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "            true_predictions = [\n",
    "                [train_dataset.id2tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            true_labels = [\n",
    "                [train_dataset.id2tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "            \n",
    "            report = seq_classification_report(true_labels, true_predictions, output_dict=True, zero_division=0)\n",
    "            f1 = seq_f1_score(true_labels, true_predictions, zero_division=0)\n",
    "\n",
    "            return {\n",
    "                \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "                \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "                \"f1\": f1,\n",
    "            }\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./bert_ner_results',\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps= 10,\n",
    "            eval_strategy= \"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForTokenClassification(self.tokenizer)\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics= compute_metrics\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer.train()\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_results = trainer.evaluate()\n",
    "        \n",
    "        # Log experiment\n",
    "        hyperparams = training_args.to_dict()\n",
    "        \n",
    "        metrics = {\n",
    "            'eval_loss': eval_results['eval_loss'],\n",
    "            'f1_score': eval_results.get('eval_f1', 0)\n",
    "        }\n",
    "        \n",
    "        self.experiment_tracker.log_experiment(\n",
    "            'BERT_Training', 'bert-base-uncased', hyperparams, metrics\n",
    "        )\n",
    "        \n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle again for safety\n",
    "bio_df = bio_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split by sentence IDs\n",
    "train_ids = bio_df['sentence_id'].unique()[:600]\n",
    "test_ids = bio_df['sentence_id'].unique()[600:]\n",
    "\n",
    "train_df = bio_df[bio_df['sentence_id'].isin(train_ids)]\n",
    "test_df = bio_df[bio_df['sentence_id'].isin(test_ids)]\n",
    "\n",
    "ner_model = NERModel('Experiment_with_bert_N1','bert-base-uncased', ExperimentTracker())\n",
    "\n",
    "train_texts, train_tags = ner_model.prepare_data(train_df)\n",
    "test_texts, test_tags = ner_model.prepare_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 07:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>0.163687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>0.103380</td>\n",
       "      <td>0.346283</td>\n",
       "      <td>0.059072</td>\n",
       "      <td>0.108213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.076910</td>\n",
       "      <td>0.539467</td>\n",
       "      <td>0.344937</td>\n",
       "      <td>0.434840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 'BERT_Training' logged successfully\n"
     ]
    }
   ],
   "source": [
    "trainer = ner_model.train_model(train_texts, train_tags, test_texts, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER INFERENCE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {'B-PER': 'persons', 'I-PER': 'persons',\n",
    "             'B-ORG': 'organizations', 'I-ORG': 'organizations',\n",
    "             'B-LOC': 'locations', 'I-LOC': 'locations'}\n",
    "\n",
    "class NERInferencePipeline:\n",
    "    def __init__(self, model_dir: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "        self.model.eval()\n",
    "        self.label_map = self.model.config.id2label\n",
    "\n",
    "    def predict(self, text: str):\n",
    "        tokens = word_tokenize(text)\n",
    "        inputs = self.tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs).logits\n",
    "        preds = torch.argmax(outputs, dim=-1).squeeze().tolist()\n",
    "\n",
    "        word_ids = inputs.word_ids()\n",
    "        grouped_preds = {}\n",
    "        current_entity = \"\"\n",
    "        current_type = \"\"\n",
    "        entities = {'persons': [], 'organizations': [], 'locations': []}\n",
    "\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                continue\n",
    "            label = self.label_map[preds[idx]]\n",
    "            if label == \"O\":\n",
    "                if current_entity and current_type:\n",
    "                    entities[current_type].append(current_entity.strip())\n",
    "                current_entity = \"\"\n",
    "                current_type = \"\"\n",
    "            else:\n",
    "                entity_type = LABEL_MAP.get(label, \"\")\n",
    "                if label.startswith(\"B-\") or entity_type != current_type:\n",
    "                    if current_entity and current_type:\n",
    "                        entities[current_type].append(current_entity.strip())\n",
    "                    current_entity = tokens[word_id]\n",
    "                    current_type = entity_type\n",
    "                else:\n",
    "                    current_entity += \" \" + tokens[word_id]\n",
    "\n",
    "        if current_entity and current_type:\n",
    "            entities[current_type].append(current_entity.strip())\n",
    "\n",
    "        return {k: \";\".join(list(dict.fromkeys(v))) for k, v in entities.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"articleBody\":\"US officials believe Russia is preparing to falsify evidence to blame Ukrainian forces for last week’s deadly blast at the Olenivka prison ahead of visits to the site by outside parties.  An administration official told CNN they expect Russia will falsify evidence, blame Ukrainian forces, and even have “reason to believe that Russia would go so far as to make it appear that Ukrainian HIMARS were to blame before journalists arrive.” John Kirby, National Security Council coordinator for strategic communications, confirmed that reporting Thursday.  “We anticipate that Russian officials will try to frame the Ukrainian Armed Forces in anticipation of journalists and potential investigators visiting the site of the attack,” Kirby said. “In fact, we’ve already seen some spurious press reports to this effect, where they have planted evidence. We have reason to believe that Russia would go so far as to make it appear that Ukrainian HIMARS – the high mobility advanced rocket systems that have been so much in the news lately – were to blame.” “And to do that before journalists arrived on site, and again, we’re beginning to even start to see some press reporting to that effect,” Kirby told CNN’s Jeremy Diamond when asked for further information on the reported plan to falsify evidence.  The US supplied Ukraine with the HIMARS multiple launch rocket system earlier this year. The US believes Russia has collected large quantities of HIMARS fragments from strikes against its positions in Ukraine and is likely to deliberately place them at sites such as the prison to claim Ukraine was responsible, according to an administration official familiar with the latest information. Ukrainian President Volodymyr Zelensky said Friday the attack on the prison in separatist-held eastern Ukraine, which resulted in the deaths of at least 50 prisoners, was “a deliberate war crime by the Russians.” Russia, meanwhile, blamed Ukraine for the attack. Kremlin spokesperson Dmitry Peskov on Thursday reiterated the Russian claim that the Ukrainian military was responsible for the attack. “There is evidence here and there is nothing to hide here,” he told CNN. “Moreover, you know that the Russian side proposed to the UN and invited the Red Cross to get acquainted with this evidence on the spot, to conduct the necessary exhaustive investigation.” The International Committee of the Red Cross confirmed Wednesday it had been able to visit the Olenivka facility only once –  in May this year to deliver water tanks. “But we did not have access to POWs held there on an individual basis – as per ICRC’s modalities of work in detention facilities – and that continues to be the case,” it said. The ICRC added: “Under the Third Geneva Convention, during international armed conflicts, the ICRC must be granted access to all PoWs, wherever they are held. We also have full liberty to choose the places we wish to visit. Since February 2022, our teams have been able to have access to some PoWs, but not all.” The ICRC says it has requested access to the detention center again since the attack last week, but has not received permission from the Russians. The Olenivka prison near Donetsk has been used to house many of the Ukrainian soldiers who surrendered at the Azovstal plant in Mariupol several months ago. CNN could not independently verify the allegations of either side. United Nations Secretary-General António Guterres said in a Wednesday news conference that the UN is seeking to establish a fact-finding team to study the attack following requests from both Russia and Ukraine to investigate. He added that the terms of reference for the panel would need to be accepted by Russia and Ukraine before the fact-finding mission would begin.  Video aired on Russian networks and shared on social media channels in Donetsk show extensive destruction to a building and several bodies. CNN was able to geolocate footage of the strike to an industrial area about two miles outside the frontline town of Olenivka. The Ukrainian military said the explosion took place on the territory of the industrial zone, in a newly-constructed building specially equipped to hold prisoners taken out of Azovstal. The Ukrainian prosecutor’s office has launched an investigation into the strike. In a statement, it said pretrial preliminary data shows “the occupying state struck the territory of penal colony No. 120 in the temporarily occupied Olenivka village of Volnovakha district of Donetsk region.” Donetsk and Luhansk are the two regions that together form Donbas, the eastern part of Ukraine where the conflict between Ukraine and Russian-backed separatists started in 2014.  The area has become the centerpiece of Russian President Vladimir Putin’s military ambition in Ukraine after his troops failed to take over Kyiv earlier this year. The Russian military has kept up a persistent barrage of artillery and missile strikes across the region for several weeks. The Kremlin says the goal of what it calls its “special military operation” is to take control of both Luhansk and Donetsk.  This story has been updated with additional details. CNN’s Barbara Starr, Tim Lister, Julia Kesaieva, Josh Pennington and Richard Roth contributed to this report. \",'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[75].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'persons': 'John Kirby;Jeremy;Zelensky', 'organizations': '', 'locations': ''}\n"
     ]
    }
   ],
   "source": [
    "inp_sent = df.iloc[75].text \n",
    "inf_pipe = NERInferencePipeline(model_dir=\"bert_ner_results/checkpoint-225\")\n",
    "res =inf_pipe.predict(inp_sent)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"articleBody\":\"US officials believe Russia is preparing to falsify evidence to blame Ukrainian forces for last week’s deadly blast at the Olenivka prison ahead of visits to the site by outside parties.  An administration official told CNN they expect Russia will falsify evidence, blame Ukrainian forces, and even have “reason to believe that Russia would go so far as to make it appear that Ukrainian HIMARS were to blame before journalists arrive.” John Kirby, National Security Council coordinator for strategic communications, confirmed that reporting Thursday.  “We anticipate that Russian officials will try to frame the Ukrainian Armed Forces in anticipation of journalists and potential investigators visiting the site of the attack,” Kirby said. “In fact, we’ve already seen some spurious press reports to this effect, where they have planted evidence. We have reason to believe that Russia would go so far as to make it appear that Ukrainian HIMARS – the high mobility advanced rocket systems that have been so much in the news lately – were to blame.” “And to do that before journalists arrived on site, and again, we’re beginning to even start to see some press reporting to that effect,” Kirby told CNN’s Jeremy Diamond when asked for further information on the reported plan to falsify evidence.  The US supplied Ukraine with the HIMARS multiple launch rocket system earlier this year. The US believes Russia has collected large quantities of HIMARS fragments from strikes against its positions in Ukraine and is likely to deliberately place them at sites such as the prison to claim Ukraine was responsible, according to an administration official familiar with the latest information. Ukrainian President Volodymyr Zelensky said Friday the attack on the prison in separatist-held eastern Ukraine, which resulted in the deaths of at least 50 prisoners, was “a deliberate war crime by the Russians.” Russia, meanwhile, blamed Ukraine for the attack. Kremlin spokesperson Dmitry Peskov on Thursday reiterated the Russian claim that the Ukrainian military was responsible for the attack. “There is evidence here and there is nothing to hide here,” he told CNN. “Moreover, you know that the Russian side proposed to the UN and invited the Red Cross to get acquainted with this evidence on the spot, to conduct the necessary exhaustive investigation.” The International Committee of the Red Cross confirmed Wednesday it had been able to visit the Olenivka facility only once –  in May this year to deliver water tanks. “But we did not have access to POWs held there on an individual basis – as per ICRC’s modalities of work in detention facilities – and that continues to be the case,” it said. The ICRC added: “Under the Third Geneva Convention, during international armed conflicts, the ICRC must be granted access to all PoWs, wherever they are held. We also have full liberty to choose the places we wish to visit. Since February 2022, our teams have been able to have access to some PoWs, but not all.” The ICRC says it has requested access to the detention center again since the attack last week, but has not received permission from the Russians. The Olenivka prison near Donetsk has been used to house many of the Ukrainian soldiers who surrendered at the Azovstal plant in Mariupol several months ago. CNN could not independently verify the allegations of either side. United Nations Secretary-General António Guterres said in a Wednesday news conference that the UN is seeking to establish a fact-finding team to study the attack following requests from both Russia and Ukraine to investigate. He added that the terms of reference for the panel would need to be accepted by Russia and Ukraine before the fact-finding mission would begin.  Video aired on Russian networks and shared on social media channels in Donetsk show extensive destruction to a building and several bodies. CNN was able to geolocate footage of the strike to an industrial area about two miles outside the frontline town of Olenivka. The Ukrainian military said the explosion took place on the territory of the industrial zone, in a newly-constructed building specially equipped to hold prisoners taken out of Azovstal. The Ukrainian prosecutor’s office has launched an investigation into the strike. In a statement, it said pretrial preliminary data shows “the occupying state struck the territory of penal colony No. 120 in the temporarily occupied Olenivka village of Volnovakha district of Donetsk region.” Donetsk and Luhansk are the two regions that together form Donbas, the eastern part of Ukraine where the conflict between Ukraine and Russian-backed separatists started in 2014.  The area has become the centerpiece of Russian President Vladimir Putin’s military ambition in Ukraine after his troops failed to take over Kyiv earlier this year. The Russian military has kept up a persistent barrage of artillery and missile strikes across the region for several weeks. The Kremlin says the goal of what it calls its “special military operation” is to take control of both Luhansk and Donetsk.  This story has been updated with additional details. CNN’s Barbara Starr, Tim Lister, Julia Kesaieva, Josh Pennington and Richard Roth contributed to this report. \",\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Vladimir Putin', 'Volodymyr Zelensky']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inp_sent)\n",
    "df.iloc[75].persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Cnn', 'Cnn', 'Cnn', 'Cnn', 'United Nations'],\n",
       " ['Russian,Volnovakha,Kyiv,Kremlin,Ukrainian,Azovstal,Olenivka,Donbas,Washington,Russia,Ukraine,Luhansk,Russians'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[75].persons ,df.iloc[75].organizations , df.iloc[75].locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Vladimir Putin,2487;Volodymyr Zelensky,602',\n",
       " 'Cnn,29;Cnn,261;Cnn,1007;Cnn,1596;United Nations,1099',\n",
       " 'Russian,Volnovakha,Kyiv,Kremlin,Ukrainian,Azovstal,Olenivka,Donbas,Washington,Russia,Ukraine,Luhansk,Russians')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[75].persons ,df.iloc[75].organizations , df.iloc[75].locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NERModel:\n",
    "#     def __init__(self, model_name: str, experiment_tracker: ExperimentTracker):\n",
    "#         self.model_name = model_name\n",
    "#         self.tokenizer = None\n",
    "#         self.model = None\n",
    "#         self.experiment_tracker = experiment_tracker\n",
    "        \n",
    "#     def prepare_data(self, bio_df: pd.DataFrame):\n",
    "#         \"\"\"Prepare data for training\"\"\"\n",
    "#         # Group by sentence_id\n",
    "#         sentences = []\n",
    "#         sentence_tags = []\n",
    "        \n",
    "#         for sent_id in bio_df['sentence_id'].unique():\n",
    "#             sent_data = bio_df[bio_df['sentence_id'] == sent_id]\n",
    "#             sentences.append(sent_data['word'].tolist())\n",
    "    \n",
    "#     def train_roberta_model(self, train_texts, train_tags, val_texts, val_tags):\n",
    "#         \"\"\"Train RoBERTa model\"\"\"\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "        \n",
    "#         train_dataset = NERDataset(train_texts, train_tags, self.tokenizer)\n",
    "#         val_dataset = NERDataset(val_texts, val_tags, self.tokenizer)\n",
    "        \n",
    "#         self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "#             'roberta-base',\n",
    "#             num_labels=len(train_dataset.tag2id)\n",
    "#         )\n",
    "        \n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir='./roberta_ner',\n",
    "#             num_train_epochs=4,\n",
    "#             per_device_train_batch_size=16,\n",
    "#             per_device_eval_batch_size=64,\n",
    "#             warmup_steps=1000,\n",
    "#             weight_decay=0.01,\n",
    "#             learning_rate=2e-5,\n",
    "#             logging_dir='./logs',\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             load_best_model_at_end=True,\n",
    "#         )\n",
    "        \n",
    "#         data_collator = DataCollatorForTokenClassification(self.tokenizer)\n",
    "        \n",
    "#         trainer = Trainer(\n",
    "#             model=self.model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=val_dataset,\n",
    "#             data_collator=data_collator,\n",
    "#             tokenizer=self.tokenizer,\n",
    "#         )\n",
    "        \n",
    "#         trainer.train()\n",
    "#         eval_results = trainer.evaluate()\n",
    "        \n",
    "#         hyperparams = {\n",
    "#             'model': 'roberta-base',\n",
    "#             'epochs': 4,\n",
    "#             'batch_size': 16,\n",
    "#             'learning_rate': 2e-5,\n",
    "#             'warmup_steps': 1000\n",
    "#         }\n",
    "        \n",
    "#         metrics = {\n",
    "#             'eval_loss': eval_results['eval_loss'],\n",
    "#             'f1_score': eval_results.get('eval_f1', 0)\n",
    "#         }\n",
    "        \n",
    "#         self.experiment_tracker.log_experiment(\n",
    "#             'RoBERTa_Enhanced', 'roberta-base', hyperparams, metrics\n",
    "#         )\n",
    "        \n",
    "#         return trainer\n",
    "\n",
    "# class AdvancedNERModel:\n",
    "#     def __init__(self, experiment_tracker: ExperimentTracker):\n",
    "#         self.experiment_tracker = experiment_tracker\n",
    "        \n",
    "#     def train_deberta_model(self, train_texts, train_tags, val_texts, val_tags):\n",
    "#         \"\"\"Train DeBERTa model (state-of-the-art)\"\"\"\n",
    "#         tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "        \n",
    "#         train_dataset = NERDataset(train_texts, train_tags, tokenizer)\n",
    "#         val_dataset = NERDataset(val_texts, val_tags, tokenizer)\n",
    "        \n",
    "#         model = AutoModelForTokenClassification.from_pretrained(\n",
    "#             'microsoft/deberta-base',\n",
    "#             num_labels=len(train_dataset.tag2id)\n",
    "#         )\n",
    "        \n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir='./deberta_ner',\n",
    "#             num_train_epochs=5,\n",
    "#             per_device_train_batch_size=12,\n",
    "#             per_device_eval_batch_size=32,\n",
    "#             warmup_steps=1500,\n",
    "#             weight_decay=0.01,\n",
    "#             learning_rate=1e-5,\n",
    "#             logging_dir='./logs',\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             load_best_model_at_end=True,\n",
    "#             gradient_accumulation_steps=2,\n",
    "#         )\n",
    "        \n",
    "#         data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "        \n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=val_dataset,\n",
    "#             data_collator=data_collator,\n",
    "#             tokenizer=tokenizer,\n",
    "#         )\n",
    "        \n",
    "#         trainer.train()\n",
    "#         eval_results = trainer.evaluate()\n",
    "        \n",
    "#         hyperparams = {\n",
    "#             'model': 'microsoft/deberta-base',\n",
    "#             'epochs': 5,\n",
    "#             'batch_size': 12,\n",
    "#             'learning_rate': 1e-5,\n",
    "#             'warmup_steps': 1500,\n",
    "#             'gradient_accumulation_steps': 2\n",
    "#         }\n",
    "        \n",
    "#         metrics = {\n",
    "#             'eval_loss': eval_results['eval_loss'],\n",
    "#             'f1_score': eval_results.get('eval_f1', 0)\n",
    "#         }\n",
    "        \n",
    "#         self.experiment_tracker.log_experiment(\n",
    "#             'DeBERTa_Advanced', 'microsoft/deberta-base', hyperparams, metrics\n",
    "#         )\n",
    "        \n",
    "#         return trainer, tokenizer\n",
    "    \n",
    "#     def train_electra_model(self, train_texts, train_tags, val_texts, val_tags):\n",
    "#         \"\"\"Train ELECTRA model\"\"\"\n",
    "#         tokenizer = AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "        \n",
    "#         train_dataset = NERDataset(train_texts, train_tags, tokenizer)\n",
    "#         val_dataset = NERDataset(val_texts, val_tags, tokenizer)\n",
    "        \n",
    "#         model = AutoModelForTokenClassification.from_pretrained(\n",
    "#             'google/electra-base-discriminator',\n",
    "#             num_labels=len(train_dataset.tag2id)\n",
    "#         )\n",
    "        \n",
    "#         training_args = TrainingArguments(\n",
    "#             output_dir='./electra_ner',\n",
    "#             num_train_epochs=4,\n",
    "#             per_device_train_batch_size=16,\n",
    "#             per_device_eval_batch_size=64,\n",
    "#             warmup_steps=800,\n",
    "#             weight_decay=0.01,\n",
    "#             learning_rate=3e-5,\n",
    "#             logging_dir='./logs',\n",
    "#             evaluation_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             load_best_model_at_end=True,\n",
    "#         )\n",
    "        \n",
    "#         data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "        \n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=training_args,\n",
    "#             train_dataset=train_dataset,\n",
    "#             eval_dataset=val_dataset,\n",
    "#             data_collator=data_collator,\n",
    "#             tokenizer=tokenizer,\n",
    "#         )\n",
    "        \n",
    "#         trainer.train()\n",
    "#         eval_results = trainer.evaluate()\n",
    "        \n",
    "#         hyperparams = {\n",
    "#             'model': 'google/electra-base-discriminator',\n",
    "#             'epochs': 4,\n",
    "#             'batch_size': 16,\n",
    "#             'learning_rate': 3e-5,\n",
    "#             'warmup_steps': 800\n",
    "#         }\n",
    "        \n",
    "#         metrics = {\n",
    "#             'eval_loss': eval_results['eval_loss'],\n",
    "#             'f1_score': eval_results.get('eval_f1', 0)\n",
    "#         }\n",
    "        \n",
    "#         self.experiment_tracker.log_experiment(\n",
    "#             'ELECTRA_Model', 'google/electra-base-discriminator', hyperparams, metrics\n",
    "#         )\n",
    "        \n",
    "#         return trainer, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
